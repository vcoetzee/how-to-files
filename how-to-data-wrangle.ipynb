{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfa3f16a",
   "metadata": {},
   "source": [
    "# How to data wrangle using python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26856c83",
   "metadata": {},
   "source": [
    "This is a detailed, step-by-step guide on how to complete data wrangling steps using Python. This guide is aimed at beginners, using Microsoft Windows, that have data in a mySQL database.\n",
    "\n",
    "Note: I started with a good quality dataset, so most of the very basic data cleanup steps (such as finding duplicates, replacing headers, renaming missing values etc.) have already been completed. If you still need to complete these steps see: Kazil, Jacqueline, and Katharine Jarmul. Data wrangling with python: tips and tools to make your life easier. \" O'Reilly Media, Inc.\", 2016.\n",
    "\n",
    "Terminology used in this how to guide:\n",
    "\n",
    "Name of the mySQL database = databasename\n",
    "Name of the table = tablename\n",
    "Name of the variable = variablename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a878d03",
   "metadata": {},
   "source": [
    "## 1. Install and import the necesary packages and libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2911ef7c",
   "metadata": {},
   "source": [
    "I already have the most recent versions of **pandas, sqlalchemy, numpy, seaborn and matplotlib** installed, but you can install them using pip (see pypi.org) or conda install in Anaconda prompt (see anaconda.org). If you get the ImportError: cannot import name 'html5lib' from 'pip._vendor', you can install html5lib in Anaconda prompt (conda install -c anaconda html5lib).\n",
    "\n",
    "Currently installed versions: \n",
    "<br>Pandas 1.4.4\n",
    "<br>sqalchemy 1.4.39\n",
    "<br>numpy 1.21.5\n",
    "<br>seaborn 0.12.2\n",
    "<br>matplotlib 3.5.1\n",
    "<br>scikit learn 1.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def244e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy as sql\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590fc3b2",
   "metadata": {},
   "source": [
    "## 2. Connect to the mySQL database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c723e67c",
   "metadata": {},
   "source": [
    "You can connect with the mySQL database using the code below. If you are unsure about your username, host or port you can easily get them using mySQL workbench. They are available in the grey block under mySQL connections with username in the first row and host:port in the second row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4709f4c6",
   "metadata": {},
   "source": [
    "## 3. Read mySQL data into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1732847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a connection with the database using:\n",
    "# url='mysql+mysqlconnector://username:password@host:port/database'\n",
    "url='mysql+mysqlconnector://root:vC97127460@localhost:3306/nutrition' #Replace XXXX with the correct username and password\n",
    "engine = sql.create_engine(url)\n",
    "\n",
    "#Read mySQL data into a pandas dataframe\n",
    "with engine.connect() as conn:\n",
    "    df = pd.read_sql('SELECT * FROM tablename', con = conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab876489",
   "metadata": {},
   "source": [
    "## 4. Check whether the data was succesfully read into the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d9d401",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Get a snapshot of the headers + first five lines of the table\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4490b42",
   "metadata": {},
   "source": [
    "## 4. Check the basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e315f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the number of observations (rows) and variables (columns/ attributes)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe04708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provides info on the number of observations (rows) and variables (columns/ attributes) + Non-Null Count and Data type\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b5532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provides info on the count, mean, std, min, 25%, 50%, 75%, and max for each column\n",
    "df.describe(include = 'all')\n",
    "# Check your min values. Are there any columns that have a min value of '0' where it is not possible, indicating possible misscoded 'missing variables'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfa24b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the incorrect '0' values with np.nan values\n",
    "df['variablename'].replace(0.000000, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43304af1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Check whether the values were replaced\n",
    "df.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eeeda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If there is a typo in a variable name you can use this code to correct it\n",
    "df=df.rename(columns = {'oldvariablename':'newvariablename'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65278508",
   "metadata": {},
   "source": [
    "## 5. Identify the number of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6d7d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the number of missing values for each column\n",
    "missing_data = df.isnull()\n",
    "for column in missing_data.columns.values.tolist():\n",
    "    print(column)\n",
    "    print (missing_data[column].value_counts())\n",
    "    print(\"\") \n",
    "    \n",
    "# An alternative option is to use the:\n",
    "df.isnull().sum()/df.shape[0]*100 \n",
    "# It is a quick way to identify columns with the highest number of missing values. \n",
    "# But because it only uses one decimal, columns with very few missing values are not identified in large datasets (i.e. they have 0.0% missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebe980e",
   "metadata": {},
   "source": [
    "## 6. Drop columns and rows (where necesary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af4908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To drop an entire row, for instance drop whole row with empty cell in \"sex\" column\n",
    "df.dropna(subset=[\"sex\"], axis=0, inplace=True) # axis=0 drops the rows\n",
    "#inplace=True (modification done on the dataset directly i.e. changes dataset)\n",
    "\n",
    "# reset index, because we dropped the row\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec25d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the row was dropped\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5649d3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recheck the number of missing values\n",
    "missing_data = df.isnull()\n",
    "for column in missing_data.columns.values.tolist():\n",
    "    print(column)\n",
    "    print (missing_data[column].value_counts())\n",
    "    print(\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bbe237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To drop columns\n",
    "df = df.drop(['variablename1', 'variablename2'], axis=1) # axis=1 drops the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48080d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the column(s) was dropped\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac006b5",
   "metadata": {},
   "source": [
    "## 8. Replace missing numerical data with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eb0c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the mean value for a variable with missing numerical data\n",
    "avg_variable = df[\"variablename\"].astype(\"float\").mean(axis=0)\n",
    "print(\"variablename:\", avg_variable)\n",
    "\n",
    "#Replace the missing value with the average value\n",
    "df[\"variablename\"].replace(np.nan, avg_variable, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a2a084",
   "metadata": {},
   "source": [
    "## 9. Replace missing categorical data with the most common value for that variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc257333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the most common value of a variable\n",
    "most_common_variable = df['variablename'].value_counts().idxmax()\n",
    "print(\"Most common variablename:\", most_common_variable)\n",
    "\n",
    "# Replace the missing value\n",
    "df[\"variablename\"].replace(np.nan, most_common_variable, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9903ca",
   "metadata": {},
   "source": [
    "## 10. Final check to see if all missing values replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f19cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the number of missing variables again\n",
    "missing_data = df.isnull()\n",
    "for column in missing_data.columns.values.tolist():\n",
    "    print(column)\n",
    "    print (missing_data[column].value_counts())\n",
    "    print(\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bffeca8",
   "metadata": {},
   "source": [
    "## 11. Check if all data is in the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7b80cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the assigned data types for all columns\n",
    "df.dtypes\n",
    "\n",
    "# If the rows are truncated so we can't see the full list, you can correct that with:\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "#Let's display max columns too \n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1740563",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change data type to bool\n",
    "df[['variablename1', 'variablename2']] = df[['variablename1', 'variablename2']].astype('bool')\n",
    "\n",
    "# Change data type to integer\n",
    "df[['variablename1', 'variablename2']] = df[['variablename1', 'variablename2']].astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff20d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check whether the data type of these variables were changed\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c559902",
   "metadata": {},
   "source": [
    "## 12. Changing variables to a standard format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2729527b",
   "metadata": {},
   "source": [
    "This step is used to standardise data into a common format. For instance, changing miles per hour to kilometers per hour. This can be done mathematically as in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9232bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change miles per hour to kilometers per hour\n",
    "df['km_hour'] = 1.609/df['miles_hour']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee4081a",
   "metadata": {},
   "source": [
    "## 13. Save cleaned dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed279e8",
   "metadata": {},
   "source": [
    "Congratulations! You now have a cleaned dataset. It is a good idea to save a copy of the data at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592f1224",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('main_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339efb87",
   "metadata": {},
   "source": [
    "## 14. Visualising the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e91077a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Basic evaluation of variables (as before)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4417dd6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotted histograms to get a feel for the data, especially to evaluate outliers and normality of distributions\n",
    "df.hist(bins=30, figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7781e4",
   "metadata": {},
   "source": [
    "## 15. Data transformation: Numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf8be6d",
   "metadata": {},
   "source": [
    "### 15.1 Dimension reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab12951",
   "metadata": {},
   "source": [
    "The \"curse of dimensionality\".\n",
    "\n",
    "Machine learning excels at analyzing data with many dimensions (variables), but it becomes more challenging to create meaningful models as the number of dimensions increase. More dimensions increase the computational efforts, the amount of training data needed to make meaningful data models and can lead to overfitting. See https://builtin.com/data-science/curse-dimensionality for a more detailed explanation.\n",
    "\n",
    "It therefore makes sense to reduce the number of variables, especially highly correlated ones, in a dataset. You can reduce variables by:\n",
    "\n",
    "* Droppoing variables that are not useful for the analyses\n",
    "* Dropping highly correlated variables\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb88c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate whether variables are highly correlated\n",
    "\n",
    "# Perform a correlation to see if you would like to combine them to reduce dimensions\n",
    "df[['variablename1', 'variablename2']].corr()\n",
    "\n",
    "#The relationship can also be vidsualised with a scatterplot\n",
    "data=df[['variablename1', 'variablename2']]\n",
    "sns.scatterplot(data, x='variablename1', y='variablename2')\n",
    "plt.show()\n",
    "\n",
    "# OR if you would like to look at a whole range of variables, but not the full df\n",
    "# Select the subset you want to evaluate\n",
    "df_subset1 = df.loc[:, 'variablename5':'variablename20']\n",
    "\n",
    "#Visualise the pairwise relationships\n",
    "sns.pairplot(df_subset1)\n",
    "plt.show()\n",
    "\n",
    "#Calculate the correlations between them\n",
    "df_subset1.corr()\n",
    "\n",
    "#OR if you want to rank the most highly correlated variables in a subset\n",
    "def get_redundant_pairs(df_subset1):\n",
    "    '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df_subset1.columns\n",
    "    for i in range(0, df_subset1.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "def get_top_abs_correlations(df_subset1, n=5):\n",
    "    au_corr = df_subset1.corr().abs().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df_subset1)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr[0:n]\n",
    "\n",
    "print(\"Top Absolute Correlations\")\n",
    "print(get_top_abs_correlations(df_subset1, 20)) # The number indicates the number of top correlations you would like to retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d609ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some options to reduce the number of variables (although many others exist)\n",
    "\n",
    "# Drop variables, especially highly correlated ones\n",
    "df = df.drop(['variablename1', 'variablename2'], axis=1) # axis=1 drops the columns\n",
    "\n",
    "# Combining variables, with or without weighting. For instance, combining moderate and strenious exercise into a single \"exercise\" variable\n",
    "df['variablename3']=df['variablename1']+(2*df['variablename2']) #weighted\n",
    "df['variablename4']=df['variablename1']+df['variablename2']+df['variablename3'] #not weighted\n",
    "\n",
    "# Variables can be averaged\n",
    "df['variablename3'] = df[['variablename1', 'variablename2']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31970b1",
   "metadata": {},
   "source": [
    "### 15.2 Distribution and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c773e8d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Whether (and how to) deal with outliers and distribution is a hotly debated topic. There is even debate about what could be considered an outlier (as opposed to an \"influential point\" etc.)\n",
    "# Your decision on how to approach this will be informed by the type of analyses you want to perform and your own views on the topic\n",
    "# Here I merely provide you with some tools to evaluate outliers and the distribution, should you wish to do so\n",
    "\n",
    "# Evaluate outliers and the distribution of the variable\n",
    "# Histogram - to visualise the distribution of the variable\n",
    "print(df['variablename'].hist())\n",
    "\n",
    "#Skewness value - Evaluates whether the variable is normally distributed or not. Any value <-1 or >1 = indicates a non-normal distribution.\n",
    "print('skewness value of variablename: ',df['variablename'].skew())\n",
    "\n",
    "#Outliers identified by IQR method - Preferred method if the distribution is skewed\n",
    "Q1 = df['variablename'].quantile(0.25)\n",
    "Q3 = df['variablename'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "whisker_width = 1.5\n",
    "variablename_outliers = df[(df['variablename'] < Q1 - whisker_width*IQR) | (df['variablename'] > Q3 + whisker_width*IQR)]\n",
    "print('IQR outliers:', variablename_outliers['variablename'].head())\n",
    "\n",
    "# Outliers identified by the standard deviation method - Preferred method if the distribution is normal\n",
    "variablename_mean = df['variablename'].mean()\n",
    "variablename_std = df['variablename'].std()\n",
    "low= variablename_mean -(3 * variablename_std)\n",
    "high= variablename_mean + (3 * variablename_std)\n",
    "variablename_outliers = df[(df['variablename'] < low) | (df['variablename'] > high)]\n",
    "print('Std outliers:', variablename_outliers['variablename'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b7a126",
   "metadata": {},
   "source": [
    "#### 15.2.1 If outliers are present, but not possible (for instance, systolic blood pressure of 640) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4c5b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to deal with outliers and obtain a normal distribution for a variable\n",
    "# Ideally you want to go back to the raw data and identify and correct the wrong entry (but this is often not possible)\n",
    "# My preferred method if outliers are present, but not possible:\n",
    "# Replace the outlier with the mean (numerical variable)\n",
    "df['variablename2']=df['variablename']\n",
    "df['variablename2'].replace(640.0, variablename_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d052db68",
   "metadata": {},
   "source": [
    "#### 15.2.2 If outliers are present (and possible) but the distribution normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e442494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to deal with outliers and obtain a normal distribution for a variable\n",
    "# My preferred method if outliers are present (and possible) but the distribution normal:\n",
    "# Outliers are capped at mean +- 3std (depending on which side of the distribution the outlier lies)\n",
    "\n",
    "# Create a new variable with outliers capped at +3std\n",
    "df['variablename_capped'] = df['variablename'] #Create a new variable in case you decide to use the initial variable later\n",
    "\n",
    "#Replace the outlier values\n",
    "df.loc[df['variablename_capped'] > high, 'variablename_capped'] = high #If you are capping at -3std use the \"low\" value from 15.1\n",
    "\n",
    "#Check the distribution after outliers capped\n",
    "df['variablename_capped'].hist()\n",
    "print('skewness:', df['variablename_capped'].skew())\n",
    "# I specifically do not test for outliers again using the IQR or Std method after the first round, because the idea is just to deal with major outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dd06a1",
   "metadata": {},
   "source": [
    "#### 15.2.3 If outliers are present (and possible) and the distribution positively skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac031fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to deal with outliers and obtain a normal distribution for a variable\n",
    "# My preferred method if outliers are present (and possible) and the distribution positively skewed:\n",
    "# Log-transformation\n",
    "df['variablename2']=df['variablename']+1 # only necesary if the variable contains 0s, because cannot log transform 0 values\n",
    "df['variablename_log']=np.log(df['variablename']) # Use variablename2 instead of variablename if the variable contained 0s\n",
    "# df['variablename_log10']=np.log10(df['variablename']) # log10 transform is theoretically more prowerful than the log transform, but in my experience log10 does not improve the distribution much compared to log\n",
    "\n",
    "# Check if outliers and distribution improved after log-transform\n",
    "# Histogram \n",
    "print(df['variablename_log'].hist())\n",
    "\n",
    "#skewness value.\n",
    "print('skewness value of variablename_log: ', df['variablename_log'].skew())\n",
    "\n",
    "#Outliers identified by IQR method\n",
    "Q1 = df['variablename_log'].quantile(0.25)\n",
    "Q3 = df['variablename_log'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "whisker_width = 1.5\n",
    "variablename_log_outliers = df[(df['variablename_log'] < Q1 - whisker_width*IQR) | (df['variablename_log'] > Q3 + whisker_width*IQR)]\n",
    "print('IQR outliers:', variablename_log_outliers['variablename_log'].head())\n",
    "\n",
    "# Outliers identified by the standard deviation method\n",
    "variablename_log_mean = df['variablename_log'].mean()\n",
    "variablename_log_std = df['variablename_log'].std()\n",
    "low= variablename_log_mean -(3 * variablename_log_std)\n",
    "high= variablename_log_mean + (3 * variablename_log_std)\n",
    "variablename_log_outliers = df[(df['variablename_log'] < low) | (df['variablename_log'] > high)]\n",
    "print('Std outliers:', variablename_log_outliers['variablename_log'].head())\n",
    "\n",
    "# If log-transform did not satisfactorily address the distribution or initial outliers\n",
    "# You could also cap the log-transformed variable using the std method (as before) or the IQR method (see below)\n",
    "# I specifically do not test for outliers again using the IQR or Std method after the first round, because the idea is just to deal with major outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2b1bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers are capped at Q1-1.5IQR or Q3+1.5IQR (depending on which end of the spectrum the outlier lies) if the log-transformation did not satisfactorily address the distribution or initial outliers\n",
    "# Capping the log-transformed values with the IQR method (only if necesary)\n",
    "# Create a new variable with outliers capped at Q3 + 1.5*IQR\n",
    "\n",
    "#Create a new variable\n",
    "df['variablename_log_capped'] = df['variablename_log']\n",
    "\n",
    "#Define the cap\n",
    "IQR_high = Q3 + whisker_width*IQR\n",
    "\n",
    "#Replace the outlier values\n",
    "df.loc[df['variablename_log_capped'] > IQR_high, 'variablename_log_capped'] = IQR_high\n",
    "\n",
    "#Check the distribution after outliers capped\n",
    "df['variablename_log_capped'].hist()\n",
    "print('skewness:', df['variablename_log_capped'].skew())\n",
    "# I specifically do not test for outliers again using the IQR or Std method after the first round, because the idea is just to deal with major outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783a5701",
   "metadata": {},
   "source": [
    "### 16. Data Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446583fc",
   "metadata": {},
   "source": [
    "Standardization is an important technique that is mostly performed as a pre-processing step before many machine learning models, to standardize the range of features of an input data set. Differences in the ranges of initial features cause trouble for many machine learning models. For example, for the models that are based on distance computation, if one of the features has a broad range of values, the distance will be governed by this particular feature.\n",
    "\n",
    "See https://builtin.com/data-science/when-and-why-standardize-your-data for more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d622eeb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start by using df.columns to select all the columns in the dataset (it makes it easier to select all the relevant columns)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c90e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select (and if necesary order) all the df columns you want to standardize\n",
    "df_num=df[['variablename1', 'variablename3', 'variablename2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc113f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform MinMax Scaling\n",
    "# Define data and scaler\n",
    "data = df_num\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# scale features\n",
    "model=scaler.fit(data) # Computes the mean and std dev for each variable so that it can be used further for scaling.\n",
    "scaled_data=model.transform(data) # Performs scaling using mean and std dev calculated using the .fit() method.\n",
    " \n",
    "# print scaled features\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44342271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the new columns (i.e. \"scaled_data\" numpy array) to df (you could also replace the values in existing df columns but we will add since we want to directly compare them)\n",
    "df2 = pd.concat([df, pd.DataFrame(scaled_data)], axis=1)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3895c468",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the new columns\n",
    "df2.columns = ['variablename1', 'variablename3', 'variablename2']\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36148e41",
   "metadata": {},
   "source": [
    "## 17. Check Colinearity in the full set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b649503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the correlations in the full set\n",
    "df2.corr()\n",
    "\n",
    "# Identify the most highly correlated variables\n",
    "def get_redundant_pairs(df2):\n",
    "    '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df2.columns\n",
    "    for i in range(0, df2.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "def get_top_abs_correlations(df2, n=5):\n",
    "    au_corr = df2.corr().abs().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df2)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr[0:n]\n",
    "\n",
    "print(\"Top Absolute Correlations\")\n",
    "print(get_top_abs_correlations(df2, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3492171f",
   "metadata": {},
   "source": [
    "## 18. Data Transformation: Categorical (ordinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee8dd43",
   "metadata": {},
   "source": [
    "Many machine learning algorithms cannot operate on label data directly. They require all input variables and output variables to be numeric. For ordinal variables (i.e. categorical variables that can be ordered or ranked) we perform ordinal encoding, if this hasn't been done already. In ordinal encoding, each unique category value is assigned an integer value. For instance, low_income =1, medium_income=2 and high_income=3.\n",
    "\n",
    "My ordinal variables are normally already numerically coded, but you can find more info about ordinal encoding here: https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed18084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also evaluate ordinal variables using a histogram, median, min, max etc,\n",
    "# Histogram \n",
    "print(df['variablename'].hist())\n",
    "\n",
    "#Median\n",
    "print('median:', df['variablename'].median())\n",
    "\n",
    "#Range\n",
    "print('min:', df['variablename'].min())\n",
    "print('max:', df['variablename'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d06833",
   "metadata": {},
   "source": [
    "## 19. Data Transformation: Categorical (nominal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c3751f",
   "metadata": {},
   "source": [
    "Nominal variables are categorical variables that cannot be ordered or ranked. They have to be one-hot-encoded because numerical coding (as above) can be mislaeding to the model. Forcing an ordinal relationship via an ordinal encoding and allowing the model to assume a natural ordering between categories may result in poor performance or unexpected results (predictions halfway between categories).\n",
    "\n",
    "For more info on one-hot-encoding see here: https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/\n",
    "\n",
    "Note: Boolean (True, False) and Binary (0,1) variables have in effect already been one-hot-encoded, so don't need to be one-hot-encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb09e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define data and one-hot-encoding\n",
    "data=df[['variablename']]\n",
    "encoder=OneHotEncoder(sparse=False)\n",
    "\n",
    "#Transform data\n",
    "onehot=encoder.fit_transform(data)\n",
    "print(onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7fe2ab",
   "metadata": {},
   "source": [
    "## 20. Save the cleaned and transformed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7c7084",
   "metadata": {},
   "source": [
    "Congratulations! You now have a cleaned and transformed dataset. It is a good idea to save a copy of the data at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba3d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('main_transformed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
