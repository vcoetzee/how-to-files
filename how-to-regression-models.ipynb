{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfa3f16a",
   "metadata": {},
   "source": [
    "# How to perform regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c22557c",
   "metadata": {},
   "source": [
    "This is a detailed, step-by-step guide on how to perform the following regression models in Python.\n",
    "\n",
    "* Multiple linear regression (forward method), \n",
    "* Regression tree, \n",
    "* Random forest, \n",
    "* Gradient boosting tree\n",
    "* Support vector regression.\n",
    "\n",
    "Terminology used in this guide:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a878d03",
   "metadata": {},
   "source": [
    "## 1. Install and import the necesary packages and libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2911ef7c",
   "metadata": {},
   "source": [
    "I already have the most recent versions of **pandas, numpy, seaborn and matplotlib** installed, but you can install them using pip (see pypi.org) or conda install in Anaconda prompt (see anaconda.org). If you get the ImportError: cannot import name 'html5lib' from 'pip._vendor', you can install html5lib in Anaconda prompt (conda install -c anaconda html5lib).\n",
    "\n",
    "Currently installed versions: \n",
    "<br>Pandas 1.4.4\n",
    "<br>numpy 1.21.5\n",
    "<br>seaborn 0.12.2\n",
    "<br>matplotlib 3.5.1\n",
    "<br>scikit learn 1.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def244e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn import svm\n",
    "from tabulate import tabulate\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590fc3b2",
   "metadata": {},
   "source": [
    "## 2. Read csv file into Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba3d472",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read the csv file into the pandas dataframe\n",
    "df2 = pd.read_csv('filename.csv')\n",
    "\n",
    "# If the rows are truncated so we can't see the full list, you can correct that with:\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Let's display max columns too \n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# To see if the file loaded correctly\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ea23fc",
   "metadata": {},
   "source": [
    "## 3. Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab95c20c",
   "metadata": {},
   "source": [
    "We will use individual correlations to select features instead of sequential feature selection (see vcoetzee/compare for why I prefer individual correlations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e1314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the full list of features in the dataset\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e7f7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the variables you want to use\n",
    "df3=df2[['variable1', 'variable2','variable3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff110cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the variables most highly correlated with your dependent variable (in this case variable1)\n",
    "df3.corrwith(df3['variable1']).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc59df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the relationships\n",
    "# Scatterplot between two continious variables, with a third variable illustrated by hue (optional)\n",
    "print(sns.scatterplot(data=df3, x='variable2', y='variable1', hue='variable3'))\n",
    "\n",
    "# Boxplot between a categorical (or ordinal) variable and your continious dependent variable\n",
    "print(sns.boxplot(data=mm, x='variable4', y='variable1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0943e395",
   "metadata": {},
   "source": [
    "## 4. Forward linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5233fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define testing and traing set\n",
    "# (.values creates a numpy array)\n",
    "x = df2[['variable2', 'variable3', 'variable4']].values\n",
    "y = df2[['variable1']].values \n",
    "\n",
    "# Splitting the dataset into training and test set (80/20 split)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1)\n",
    "print ('Train set:', x_train.shape,  y_train.shape)\n",
    "print ('Test set:', x_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e4cdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model - according to correlations\n",
    "# Step 1\n",
    "x_train1 = x_train[:, [2]] # your most highly correlated x variable\n",
    "x_test1 = x_test[:, [2]]\n",
    "lr1 = LinearRegression()\n",
    "lr1.fit(x_train1, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_lr1=lr1.predict(x_test1)\n",
    "\n",
    "# Evaluate the predictions\n",
    "lr_R21 = r2_score(y_test, y_pred_lr1)\n",
    "lr_MSE1 = mean_squared_error(y_test, y_pred_lr1)\n",
    "print('Step1 R2:', lr_R21)\n",
    "print('Step1 MSE:', lr_MSE1)\n",
    "print()\n",
    "  \n",
    "#_________________________________\n",
    "# Step 2\n",
    "x_train2 = x_train[:, [2,3]]  # your two most highly correlated x variables\n",
    "x_test2 = x_test[:, [2,3]]\n",
    "lr2 = LinearRegression()\n",
    "lr2.fit(x_train2, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_lr2 = lr2.predict(x_test2)\n",
    "\n",
    "# Evaluate the predictions\n",
    "lr_R22 = r2_score(y_test, y_pred_lr2)\n",
    "lr_MSE2 = mean_squared_error(y_test, y_pred_lr2)\n",
    "print('Step2 R2:', lr_R22)\n",
    "print('Step2 MSE:', lr_MSE2)\n",
    "print()\n",
    "  \n",
    "#____________________________________\n",
    "# Step 3 (etc)\n",
    "\n",
    "# Identify the best model (i.e. the one with the highest R2 and lowest MSE)\n",
    "# I keep adding variables/ steps until R2 have peaked (i.e. the last model should have a lower R2 than the one before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aaee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for multicolinearity in your best model \n",
    "\n",
    "# Convert numpy to df\n",
    "vif_df = pd.DataFrame(x_train2, columns = ['variable2', 'variable3'])\n",
    "\n",
    "# Convert datatypes to float\n",
    "vif_df[['variable2', 'variable3']] = vif_df[['variable2', 'variable3']].astype('float')\n",
    "\n",
    "# VIF dataframe\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = vif_df.columns\n",
    "\n",
    "  \n",
    "# calculating VIF for each feature\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(vif_df.values, i)\n",
    "                          for i in range(len(vif_df.columns))]\n",
    "\n",
    "  \n",
    "print(vif_data)\n",
    "\n",
    "# Generally, a VIF above 5 indicates a high multicollinearity and a VIF above 10 needs to be corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737e0424",
   "metadata": {},
   "source": [
    "## 5. Regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea31281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the regressor\n",
    "rt = DecisionTreeRegressor(random_state=1)\n",
    "\n",
    "# Train the regressor \n",
    "rt.fit(x_train, y_train)\n",
    "\n",
    "# Predicting the test data\n",
    "y_pred_rt = rt.predict(x_test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "# Calculate R2 score (higher R2=better performance) and MSE scores (lower MSE = better performance)\n",
    "rt_R2 = r2_score(y_test, y_pred_rt)\n",
    "rt_MSE = mean_squared_error(y_test, y_pred_rt)\n",
    "print('Regression tree R2:', rt_R2)\n",
    "print('Regression tree MSE:', rt_MSE)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2bf6e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identify the ideal max_depth value to use for pruning the tree\n",
    "# (Regression trees tend to overfit, so it is important to prune the tree)\n",
    "depth=[]\n",
    "for i in range(1,5):\n",
    "    regressor = DecisionTreeRegressor(random_state=1, criterion='squared_error', max_depth=i)\n",
    "    model = regressor.fit(x_train, y_train)\n",
    "    # Perform 5-fold cross validation \n",
    "    model_scores = cross_val_score(model, x, y, cv = 3, scoring='r2')\n",
    "    print(\"mean R2 cross validation score: {}\".format(np.mean(model_scores)))\n",
    "\n",
    "    # Predict the response for test dataset\n",
    "    y_pred = model.predict(x_test)\n",
    "    depth.append(mean_squared_error(y_test, y_pred))\n",
    "print('MSE:', depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d03d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the regressor for the optimal max_depth value identified above\n",
    "rt3 = DecisionTreeRegressor(max_depth=3, random_state=1)\n",
    "\n",
    "# Train the regressor \n",
    "rt3.fit(x_train, y_train)\n",
    "\n",
    "# Predicting the test data\n",
    "y_pred_rt3 = rt3.predict(x_test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "# Calculate R2 score (higher R2=better performance) and MSE scores (lower MSE = better performance)\n",
    "rt3_R2 = r2_score(y_test, y_pred_rt3)\n",
    "rt3_MSE = mean_squared_error(y_test, y_pred_rt3)\n",
    "print('Regression tree R2:', rt3_R2)\n",
    "print('Regression tree MSE:', rt3_MSE)\n",
    "print()\n",
    "\n",
    "# plot tree\n",
    "# set plot size (denoted in inches)\n",
    "plt.figure(figsize=(15,12))\n",
    "tree.plot_tree(rt3, feature_names=['variable2', 'variable3', 'variable4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86fdc7c",
   "metadata": {},
   "source": [
    "## 4. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9080a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten y (otherwise you get the error: \n",
    "   # DataConversionWarning: A column-vector y was passed when a 1d array was expected. \n",
    "   # Please change the shape of y to (n_samples,), for example using ravel().)\n",
    "y_train1_1 = np.ravel(y_train, order = 'C')\n",
    "print('y_train shape:', y_train1_1.shape)\n",
    "y_test1_1 = np.ravel(y_test, order = 'C')\n",
    "print('y_test shape:', y_test1_1.shape)\n",
    "\n",
    "# Create the regressor\n",
    "# You can change the number of estimators\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state=1)\n",
    "\n",
    "# Train the regressor\n",
    "rf.fit(x_train, y_train1_1) \n",
    "\n",
    "# Predicting the test data\n",
    "y_pred_rf = rf.predict(x_test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "# Calculate R2 score (higher R2=better performance) and MSE scores (lower MSE = better performance)\n",
    "rf_R2 = r2_score(y_test1_1, y_pred_rf)\n",
    "rf_MSE = mean_squared_error(y_test, y_pred_rf)\n",
    "print('Random Forest R2:', rf_R2)\n",
    "print('Random Forest MSE:', rf_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6086f70",
   "metadata": {},
   "source": [
    "## 5. Gradient boosting tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98b9063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the regressor\n",
    "gb = GradientBoostingRegressor(random_state=1)\n",
    "\n",
    "# Train the regressor\n",
    "gb.fit(x_train, y_train1_1) \n",
    "\n",
    "# Predicting the test data\n",
    "y_pred_gb = gb.predict(x_test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "# Calculate R2 score (higher R2=better pegbormance) and MSE scores (lower MSE = better pegbormance)\n",
    "gb_R2 = r2_score(y_test1_1, y_pred_gb)\n",
    "gb_MSE = mean_squared_error(y_test, y_pred_rf)\n",
    "print('Gradient Boost R2:', gb_R2)\n",
    "print('Gradient Boost MSE:', gb_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442d1120",
   "metadata": {},
   "source": [
    "## 6. Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16844181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the regressor\n",
    "svr = svm.SVR()\n",
    "\n",
    "# Train the regressor\n",
    "svr.fit(x_train, y_train1_1) \n",
    "\n",
    "# Predicting the test data\n",
    "y_pred_svr = svr.predict(x_test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "# Calculate R2 score (higher R2=better pesvrormance) and MSE scores (lower MSE = better pesvrormance)\n",
    "svr_R2 = r2_score(y_test1_1, y_pred_svr)\n",
    "svr_MSE = mean_squared_error(y_test, y_pred_svr)\n",
    "print('Support vector R2:', svr_R2)\n",
    "print('Support vector MSE:', svr_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581e3199",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print a model comparison table\n",
    "print('Model comparison')\n",
    "table = [['Description', 'R2', 'MSE'], ['Linear regr', lr_R24, lr_MSE4], \n",
    "         ['Regr Tree', rt3_R2, rt3_MSE], ['Random Forest', rf_R2, rf_MSE], \n",
    "         ['Gradient Boost', gb_R2, gb_MSE], ['Support vector', svr_R2, svr_MSE]]\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978cb4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can perform hyperparameter tuning for the random forest model\n",
    "   # if you want to improve it somewhat (optional)\n",
    "\n",
    "# Create a randomised hyperparameter grid\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 500, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['log2', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b246df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "\n",
    "# First create the base model to tune\n",
    "rf2 = RandomForestRegressor()\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf2_random = RandomizedSearchCV(estimator = rf2, param_distributions = random_grid, \n",
    "                                n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf2_random.fit(x_train, y_train1_1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5d37dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters for the best model\n",
    "rf2_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a04baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the regressor\n",
    "# (Use the best parameters obtained above)\n",
    "rf2 = RandomForestRegressor(n_estimators=1000,min_samples_split=5,min_samples_leaf=4, \n",
    "                            max_features='sqrt', max_depth=30, bootstrap=False)\n",
    "\n",
    "# Train the regressor\n",
    "rf2.fit(x_train, y_train1_1) \n",
    "\n",
    "# Predicting the test data\n",
    "y_pred_rf2 = rf2.predict(x_test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "rf2_R2 = r2_score(y_test1_1, y_pred_rf2)\n",
    "rf2_MSE = mean_squared_error(y_test, y_pred_rf2)\n",
    "\n",
    "print('Random Forest R2:', rf2_R2)\n",
    "print('Random Forest MSE:', rf2_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15480114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model comparison table with the latest model included\n",
    "print('Model comparison')\n",
    "table2 = [['Description', 'R2', 'MSE'], ['Linear regr', lr_R24, lr_MSE4], \n",
    "          ['Regr Tree', rt3_R2, rt3_MSE], ['Random Forest1', rf_R2, rf_MSE], \n",
    "          ['Random Forest2', rf2_R2, rf2_MSE], ['Gradient Boost', gb_R2, gb_MSE], \n",
    "          ['Support vector', svr_R2, svr_MSE]]\n",
    "print(tabulate(table2, headers='firstrow', tablefmt='fancy_grid'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
